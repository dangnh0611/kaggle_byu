# @package _global_

defaults:
  - heatmap
  - override /model: monai_flexunet
  - _self_

model:
  weight_init: optimal_bias

loss:
  mtl: null

data:
  particles: [2]
  patch_size: [64, 320, 320]
  ensure_fg: True
  label_smooth: [0.0,1.0]

  transform:
    norm1: sample_percentile_clip
    norm2: False
    norm2_mean: 0.45
    norm2_std: 0.225
    norm2_nonzero: False

  aug:
    enable: True
    affine1_prob: 0.5
    affine1_scale: 0.1 # max_skew_xy = 1.1 / 0.9 = 1.22
    affine2_prob: 0.1
    affine2_rotate_xy: 45 # degrees
    affine2_shear: 0.05
    affine2_scale: 0.1 # max_skew_xy = 1.1 / 0.9 = 1.22
    zoom_prob: 0.0
    zoom_range: [[0.7, 1.3], [0.7, 1.3]] # (xy_zoom_range, z_zoom_range), XY share same zoom

    # no lazy, can't properly transform points
    grid_distort_prob: 0.0
    smooth_deform_prob: 0.0

    downsample_prob: 0.1
    intensity_prob: 0.1
    smooth_prob: 0.0

callbacks:
  validation_scheduler:
    milestones: [300]
    val_check_intervals: [50]
    check_val_every_n_epochs: null
    milestone_unit: step

trainer:
  max_steps: 1400
  val_check_interval: 300
  check_val_every_n_epoch: null

optim:
  name: torch@adamw
  lr: 1e-3
  weight_decay: 0.0
  betas: [0.9, 0.999]
  eps: 1e-7

scheduler:
  name: timm@cosine
  warmup_steps: 800
  warmup_epochs: null
  cooldown_epochs: 0
  min_lr_factor: 1e-2
  warmup_lr_factor: 1e-2
  cycle_limit: 1
  cycle_decay: 0.5

ema:
  enable: True
  train_decays: [0.99, 0.995]
  val_decays: [0.99, 0.995]

tags: ["heatmap", "flexunet", "r10", "beta"]
exp_name: flexunet_r10_beta



