# @package _global_

defaults:
  - override /task: 2d_spacing
  - override /data: 2d_spacing
  - override /model: 2d_spacing
  - override /loss: mse
  - _self_

env:
  data_dir: data

cv:
  strategy: UNK
  num_folds: 4
  fold_idx: 0

loader:
  train_batch_size: 1
  val_batch_size: 1
  train_num_workers: 16
  val_num_workers: 8
  pytorch_num_threads: 8
  pin_memory: False

callbacks:
  early_stopping:
    patience: 4 # number of checks with no improvement after which training will be stopped
  model_checkpoint:
    metrics: ["val/MAE"]
    train_time_interval: null
    every_n_train_steps: null
    every_n_epochs: null
    save_on_train_epoch_end: False
    save_top_k: 5
  validation_scheduler:
    milestones: []
    val_check_intervals: []
    check_val_every_n_epochs: null
    milestone_unit: step
  model_summary:
    max_depth: 4

trainer:
  max_epochs: null
  max_steps: 15000
  val_check_interval: 3000
  check_val_every_n_epoch: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  precision: 16-mixed
  deterministic: True
  benchmark: False
  log_every_n_steps: 5

optim:
  name: torch@adamw
  lr: 1e-4
  weight_decay: 0.0
  betas: [0.9, 0.999]
  eps: 1e-7
  # momentum: 0.9

scheduler:
  name: timm@cosine
  warmup_steps: 0
  warmup_epochs: null
  cooldown_epochs: 0
  min_lr_factor: 5e-3
  warmup_lr_factor: 1e-2
  cycle_limit: 1
  cycle_decay: 0.5

ema:
  enable: True
  train_decays: [0.99]
  val_decays: [0, 0.99]
  test_decays: ${ema.train_decays}
  force_cpu: False
  reset_from: global_best # null or none, primary, last_best, global_best
  reset_sched_epochs: [-1] # list of epochs to perform reset
  reset_on_plateau: -1
  min_reset_interval: 5

misc:
  log_raw_cfg: False
  log_cfg: True
  log_cfg_rich: True
  log_model: True  # print model in Torch style

loggers:
  wandb:
    project: byu

seed: 42
exp_name: byu_2d_spacing